'use strict';

var getErrorShape = require('../error/getErrorShape.js');
var TRPCError = require('../error/TRPCError.js');
var router = require('../router.js');
var stream = require('../stream/stream.js');
var transformer = require('../transformer.js');
var utils = require('../utils.js');
var contentType = require('./contentType.js');
var getHTTPStatusCode = require('./getHTTPStatusCode.js');

const HTTP_METHOD_PROCEDURE_TYPE_MAP = {
    GET: 'query',
    POST: 'mutation'
};
function initResponse(initOpts) {
    const { ctx , info , type , responseMeta , untransformedJSON , errors =[] , headers ,  } = initOpts;
    let status = untransformedJSON ? getHTTPStatusCode.getHTTPStatusCode(untransformedJSON) : 200;
    const eagerGeneration = !untransformedJSON;
    const data = eagerGeneration ? [] : Array.isArray(untransformedJSON) ? untransformedJSON : [
        untransformedJSON
    ];
    const meta = responseMeta?.({
        ctx,
        info,
        paths: info?.calls.map((call)=>call.path),
        type,
        data,
        errors,
        eagerGeneration
    }) ?? {};
    if (meta.headers) {
        if (meta.headers instanceof Headers) {
            for (const [key, value] of meta.headers.entries()){
                headers.append(key, value);
            }
        } else {
            /**
       * @deprecated, delete in v12
       */ for (const [key1, value1] of Object.entries(meta.headers)){
                if (Array.isArray(value1)) {
                    for (const v of value1){
                        headers.append(key1, v);
                    }
                } else if (typeof value1 === 'string') {
                    headers.set(key1, value1);
                }
            }
        }
    }
    if (meta.status) {
        status = meta.status;
    }
    return {
        status
    };
}
function caughtErrorToData(cause, errorOpts) {
    const { router , req , onError  } = errorOpts.opts;
    const error = TRPCError.getTRPCErrorFromUnknown(cause);
    onError?.({
        error,
        path: errorOpts.path,
        input: errorOpts.input,
        ctx: errorOpts.ctx,
        type: errorOpts.type,
        req
    });
    const untransformedJSON = {
        error: getErrorShape.getErrorShape({
            config: router._def._config,
            error,
            type: errorOpts.type,
            path: errorOpts.path,
            input: errorOpts.input,
            ctx: errorOpts.ctx
        })
    };
    const transformedJSON = transformer.transformTRPCResponse(router._def._config, untransformedJSON);
    const body = JSON.stringify(transformedJSON);
    return {
        error,
        untransformedJSON,
        body
    };
}
async function resolveResponse(opts) {
    const { router: router$1 , req  } = opts;
    const headers = new Headers([
        [
            'vary',
            'trpc-accept'
        ]
    ]);
    const config = router$1._def._config;
    const url = new URL(req.url);
    if (req.method === 'HEAD') {
        // can be used for lambda warmup
        return new Response(null, {
            status: 204
        });
    }
    const allowBatching = opts.allowBatching ?? opts.batching?.enabled ?? true;
    const allowMethodOverride = (opts.allowMethodOverride ?? false) && req.method === 'POST';
    const type = HTTP_METHOD_PROCEDURE_TYPE_MAP[req.method] ?? 'unknown';
    let ctx = undefined;
    let info = undefined;
    const isStreamCall = req.headers.get('trpc-accept') === 'application/jsonl';
    const experimentalIterablesAndDeferreds = config.experimental?.iterablesAndDeferreds ?? false;
    try {
        info = contentType.getRequestInfo({
            req,
            path: decodeURIComponent(opts.path),
            config: config,
            searchParams: url.searchParams
        });
        // we create context early so that error handlers may access context information
        ctx = await opts.createContext({
            info
        });
        if (opts.error) {
            throw opts.error;
        }
        if (info.isBatchCall && !allowBatching) {
            throw new TRPCError.TRPCError({
                code: 'BAD_REQUEST',
                message: `Batching is not enabled on the server`
            });
        }
        if (type === 'unknown') {
            throw new TRPCError.TRPCError({
                message: `Unexpected request method ${req.method}`,
                code: 'METHOD_NOT_SUPPORTED'
            });
        }
        const errors = [];
        const promises = info.calls.map(async (call)=>{
            try {
                const data = await router.callProcedure({
                    procedures: opts.router._def.procedures,
                    path: call.path,
                    getRawInput: call.getRawInput,
                    ctx,
                    type,
                    allowMethodOverride
                });
                if ((!isStreamCall || !experimentalIterablesAndDeferreds) && utils.isObject(data) && (Symbol.asyncIterator in data || Object.values(data).some(stream.isPromise))) {
                    if (!isStreamCall) {
                        throw new TRPCError.TRPCError({
                            code: 'UNSUPPORTED_MEDIA_TYPE',
                            message: 'Cannot return async iterable or nested promises in non-streaming response'
                        });
                    }
                    if (!experimentalIterablesAndDeferreds) {
                        throw new TRPCError.TRPCError({
                            code: 'INTERNAL_SERVER_ERROR',
                            message: 'Missing experimental flag "iterablesAndDeferreds"'
                        });
                    }
                }
                return {
                    result: {
                        data
                    }
                };
            } catch (cause) {
                const error = TRPCError.getTRPCErrorFromUnknown(cause);
                errors.push(error);
                const input = call.result();
                opts.onError?.({
                    error,
                    path: call.path,
                    input,
                    ctx,
                    type: type,
                    req: opts.req
                });
                return {
                    error: getErrorShape.getErrorShape({
                        config,
                        error,
                        type,
                        path: call.path,
                        input,
                        ctx
                    })
                };
            }
        });
        if (!isStreamCall) {
            headers.set('content-type', 'application/json');
            /**
       * Non-streaming response:
       * - await all responses in parallel, blocking on the slowest one
       * - create headers with known response body
       * - return a complete HTTPResponse
       */ const untransformedJSON = await Promise.all(promises);
            const errors1 = untransformedJSON.flatMap((response)=>'error' in response ? [
                    response.error
                ] : []);
            const headResponse = initResponse({
                ctx,
                info,
                type,
                responseMeta: opts.responseMeta,
                untransformedJSON,
                errors: errors1,
                headers
            });
            // return body stuff
            const result = info.isBatchCall ? untransformedJSON : untransformedJSON[0];
            const transformedJSON = transformer.transformTRPCResponse(router$1._def._config, result);
            const body = JSON.stringify(transformedJSON);
            return new Response(body, {
                status: headResponse.status,
                headers
            });
        }
        headers.set('content-type', 'application/json');
        headers.set('transfer-encoding', 'chunked');
        /**
     * Streaming response:
     * - block on none, call `onChunk` as soon as each response is ready
     * - create headers with minimal data (cannot know the response body in advance)
     * - return void
     */ const headResponse1 = initResponse({
            ctx,
            info,
            type,
            responseMeta: opts.responseMeta,
            errors: [],
            headers
        });
        const stream$1 = stream.jsonlStreamProducer({
            /**
       * Example structure for `maxDepth: 4`:
       * {
       *   // 1
       *   0: {
       *     // 2
       *     result: {
       *       // 3
       *       data: // 4
       *     }
       *   }
       * }
       */ maxDepth: experimentalIterablesAndDeferreds ? 4 : 3,
            formatError (errorOpts) {
                const call = info?.calls[errorOpts.path[0]];
                return getErrorShape.getErrorShape({
                    config,
                    ctx,
                    error: TRPCError.getTRPCErrorFromUnknown(errorOpts.error),
                    input: call?.result(),
                    path: call?.path,
                    type
                });
            },
            data: promises.map(async (it)=>{
                const response = await it;
                if ('result' in response) {
                    /**
           * Not very pretty, but we need to wrap nested data in promises
           * Our stream producer will only resolve top-level async values or async values that are directly nested in another async value
           */ return {
                        ...response,
                        result: Promise.resolve({
                            ...response.result,
                            data: Promise.resolve(response.result.data)
                        })
                    };
                }
                return response;
            }),
            serialize: config.transformer.output.serialize,
            onError: (cause)=>{
                opts.onError?.({
                    error: TRPCError.getTRPCErrorFromUnknown(cause),
                    path: undefined,
                    input: undefined,
                    ctx,
                    type,
                    req: opts.req
                });
            }
        });
        return new Response(stream$1, {
            headers,
            status: headResponse1.status
        });
    } catch (cause) {
        // we get here if
        // - batching is called when it's not enabled
        // - `createContext()` throws
        // - `router._def._config.transformer.output.serialize()` throws
        // - post body is too large
        // - input deserialization fails
        // - `errorFormatter` return value is malformed
        const { error , untransformedJSON: untransformedJSON1 , body: body1  } = caughtErrorToData(cause, {
            opts,
            ctx,
            type
        });
        const headResponse2 = initResponse({
            ctx,
            info,
            type,
            responseMeta: opts.responseMeta,
            untransformedJSON: untransformedJSON1,
            errors: [
                error
            ],
            headers
        });
        return new Response(body1, {
            status: headResponse2.status,
            headers
        });
    }
}

exports.resolveResponse = resolveResponse;
